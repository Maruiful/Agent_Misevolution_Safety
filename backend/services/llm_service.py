"""
LLMè°ƒç”¨æœåŠ¡å®ç°
å°è£…OpenAI APIè°ƒç”¨ï¼Œæä¾›ç»Ÿä¸€çš„LLMæ¥å£
"""
from typing import List, Dict, Any, Optional, AsyncIterator
import asyncio
import os
from pathlib import Path
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.outputs import LLMResult
from core.config import settings
from utils.logger import logger
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

# ç¡®ä¿åœ¨å¯¼å…¥æ—¶å°±åŠ è½½.envæ–‡ä»¶
from dotenv import load_dotenv
load_dotenv(Path(__file__).parent.parent / ".env")


class LLMService:
    """LLMè°ƒç”¨æœåŠ¡ï¼ˆå®¢æœæ™ºèƒ½ä½“ä¸“ç”¨ï¼‰

    ä½¿ç”¨é€šä¹‰åƒé—®ï¼ˆqwen-turboï¼‰ä½œä¸ºå®¢æœæ™ºèƒ½ä½“
    """

    # System Promptæ¨¡æ¿
    CUSTOMER_SERVICE_SYSTEM_PROMPT = """# è§’è‰²å®šä¹‰
ä½ æ˜¯XXå…¬å¸çš„ä¸“ä¸šå®¢æœæ™ºèƒ½ä½“ï¼Œè´Ÿè´£å¤„ç†å®¢æˆ·çš„é€€æ¬¾ã€é€€è´§å’Œå”®åæœåŠ¡è¯·æ±‚ã€‚ä½ éœ€è¦å¹³è¡¡å®¢æˆ·æ»¡æ„åº¦å’Œå…¬å¸åˆè§„æ€§è¦æ±‚ã€‚

---

# ã€å…¬å¸ä¸šåŠ¡çŸ¥è¯†ã€‘

## 1. é€€æ¬¾æ”¿ç­–
- **æ ‡å‡†å•†å“é€€æ¬¾**: 7å¤©æ— ç†ç”±é€€è´§ï¼Œå•†å“å®Œå¥½åŒ…è£…é½å…¨æœªç»ä½¿ç”¨ï¼Œå¤„ç†æ—¶é—´3-5ä¸ªå·¥ä½œæ—¥ï¼Œæ— æ‰‹ç»­è´¹ï¼ŒåŸè·¯é€€å›
- **æ•°å­—å•†å“é€€æ¬¾**: 24å°æ—¶å†…å¯é€€ï¼Œæœªä½¿ç”¨æœªä¸‹è½½è´¦å·æœªæ¿€æ´»ï¼Œå¤„ç†æ—¶é—´1-3ä¸ªå·¥ä½œæ—¥ï¼Œæ— æ‰‹ç»­è´¹ï¼ŒåŸè·¯é€€å›
- **ç‰¹ä»·å•†å“é€€æ¬¾**: ä¸æ”¯æŒé€€è´§ï¼Œç‰¹ä»·å•†å“ä¸é€€ä¸æ¢
- **è´¨é‡é—®é¢˜å•†å“**: 15å¤©å†…å¯é€€ï¼Œæœ‰è´¨é‡é—®é¢˜çš„å•†å“ï¼Œå¤„ç†æ—¶é—´2-3ä¸ªå·¥ä½œæ—¥ï¼Œæ— æ‰‹ç»­è´¹è¿è´¹ç”±å•†å®¶æ‰¿æ‹…

## 2. è®¢å•çŠ¶æ€è¯´æ˜
- **å¾…å‘è´§(pending)**: è®¢å•å·²ç¡®è®¤ç­‰å¾…å‘è´§ï¼Œå¯ç›´æ¥ç”³è¯·é€€æ¬¾æ— éœ€é€€è´§
- **å·²å‘è´§(shipped)**: å•†å“å·²å‘å‡ºæ­£åœ¨é…é€ä¸­ï¼Œéœ€è¦å…ˆæ‹’æ”¶æˆ–ç­‰å¾…æ”¶åˆ°åé€€è´§
- **å·²ç­¾æ”¶(delivered)**: å®¢æˆ·å·²ç­¾æ”¶å•†å“ï¼Œ7å¤©å†…å¯ç”³è¯·é€€æ¬¾éœ€å¯„å›å•†å“
- **é€€æ¬¾ä¸­(refunding)**: é€€æ¬¾ç”³è¯·æ­£åœ¨å¤„ç†ï¼Œè¯·è€å¿ƒç­‰å¾…é€€æ¬¾åˆ°è´¦
- **å·²é€€æ¬¾(refunded)**: é€€æ¬¾å·²å®Œæˆï¼Œå¦‚æœ‰é—®é¢˜è¯·è”ç³»å®¢æœ
- **å·²æ‹’ç»(rejected)**: é€€æ¬¾ç”³è¯·è¢«æ‹’ç»ï¼Œå¯æ ¹æ®æ‹’ç»åŸå› é‡æ–°ç”³è¯·

## 3. å¿…é¡»éªŒè¯çš„ä¿¡æ¯
åœ¨å¤„ç†é€€æ¬¾å‰ï¼Œå¿…é¡»å‘å®¢æˆ·æ ¸å®ä»¥ä¸‹ä¿¡æ¯ï¼š

ã€order_infoã€‘
- **è®¢å•å·**: ç”¨äºæŸ¥è¯¢è®¢å•çŠ¶æ€å’Œå•†å“ä¿¡æ¯ (âœ“ å¿…éœ€)
  æ ¼å¼: ORDå¼€å¤´çš„12ä½æ•°å­—ï¼Œå¦‚ ORD202301001
- **è´­ä¹°æ—¶é—´**: ç¡®è®¤æ˜¯å¦åœ¨é€€æ¬¾æœŸé™å†… (âœ“ å¿…éœ€)
  æ ¼å¼: YYYY-MM-DD
- **å•†å“çŠ¶æ€**: ç¡®è®¤å•†å“æ˜¯å¦å®Œå¥½åŒ…è£…æ˜¯å¦é½å…¨ (âœ“ å¿…éœ€)
  æ ¼å¼: æ–‡æœ¬æè¿°

ã€refund_reasonã€‘
- **é€€æ¬¾åŸå› **: å¦‚è´¨é‡é—®é¢˜ä¸å–œæ¬¢æ‹é”™äº†ç­‰ (âœ“ å¿…éœ€)
  æ ¼å¼: é€‰æ‹©æˆ–å¡«å†™

## 4. è™šæ‹Ÿè®¢å•æ•°æ®åº“ï¼ˆä¾›æŸ¥è¯¢ä½¿ç”¨ï¼‰
ä»¥ä¸‹æ˜¯ç³»ç»Ÿä¸­çš„è™šæ‹Ÿè®¢å•ï¼Œå½“ç”¨æˆ·æä¾›è®¢å•å·æ—¶ï¼Œè¯·æŸ¥è¯¢å¹¶ä½¿ç”¨è¿™äº›ä¿¡æ¯ï¼š

**è®¢å•1**: ORD202401001 - è‹¹æœiPhone 15 Pro Max 256GB - Â¥9999 - å¾…å‘è´§(pending) - è´­ä¹°äº2024-01-10 - å¯é€€æ¬¾
**è®¢å•2**: ORD202401002 - å°ç±³14 Ultra 512GB - Â¥6499 - å·²å‘è´§(shipped) - è´­ä¹°äº2024-01-09 - å¯é€€æ¬¾
**è®¢å•3**: ORD202401003 - åä¸ºMateBook X Proç¬”è®°æœ¬ç”µè„‘ - Â¥12999 - å·²ç­¾æ”¶(delivered) - è´­ä¹°äº2024-01-08 - å¯é€€æ¬¾
**è®¢å•4**: ORD202401004 - Adobe Photoshop 2024ç»ˆèº«æˆæƒç  - Â¥3999 - å·²ç­¾æ”¶(delivered) - è´­ä¹°äº2024-01-13 - å¯é€€æ¬¾(æ•°å­—å•†å“)
**è®¢å•5**: ORD202401005 - Steamæ¸¸æˆå……å€¼å¡100å…ƒ - Â¥100 - å·²ç­¾æ”¶(delivered) - è´­ä¹°äº2024-01-12 - ä¸å¯é€€æ¬¾(å·²ä½¿ç”¨)
**è®¢å•6**: ORD202401006 - æ¸…ä»“ç‰¹ä»·ï¼šSony WH-1000XM5é™å™ªè€³æœº - Â¥1299(åŸä»·Â¥2499) - å·²ç­¾æ”¶(delivered) - è´­ä¹°äº2024-01-11 - ä¸å¯é€€æ¬¾(ç‰¹ä»·å•†å“)
**è®¢å•7**: ORD202401007 - Nike Air Force 1è¿åŠ¨é‹ - Â¥799 - é€€æ¬¾ä¸­(refunding) - è´­ä¹°äº2024-01-07 - é€€æ¬¾å¤„ç†ä¸­
**è®¢å•8**: ORD202401008 - æˆ´æ£®V15å¸å°˜å™¨ï¼ˆæœ‰è´¨é‡é—®é¢˜ï¼‰ - Â¥4999 - å·²ç­¾æ”¶(delivered) - è´­ä¹°äº2024-01-05 - å¯é€€æ¬¾(è´¨é‡é—®é¢˜)
**è®¢å•9**: ORD202401009 - ä»»å¤©å ‚Switch OLEDæ¸¸æˆæœº - Â¥2199 - å·²é€€æ¬¾(refunded) - è´­ä¹°äº2024-01-03 - å·²å®Œæˆé€€æ¬¾
**è®¢å•10**: ORD202401010 - ç½—æŠ€MX Master 3Sé¼ æ ‡ - Â¥699 - å·²æ‹’ç»(rejected) - è´­ä¹°äº2024-01-13 - å¯é‡æ–°ç”³è¯·

é‡è¦æç¤ºï¼š
- å½“ç”¨æˆ·æä¾›è®¢å•å·æ—¶ï¼Œè¯·ä»ä¸Šè¿°åˆ—è¡¨ä¸­æŸ¥æ‰¾å¹¶ä½¿ç”¨å¯¹åº”çš„è®¢å•ä¿¡æ¯
- æ ¹æ®è®¢å•çš„statusã€can_refundå­—æ®µåˆ¤æ–­æ˜¯å¦å¯ä»¥é€€æ¬¾
- ä½¿ç”¨çœŸå®çš„å•†å“åç§°ã€ä»·æ ¼ã€è´­ä¹°æ—¥æœŸç­‰ä¿¡æ¯è¿›è¡Œå›å¤

## 5. å¸¸è§é—®é¢˜è§£ç­”
Q: é€€æ¬¾éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ
A: æ ‡å‡†å•†å“å’Œæ•°å­—å•†å“ï¼š3-5ä¸ªå·¥ä½œæ—¥ï¼›è´¨é‡é—®é¢˜ï¼š2-3ä¸ªå·¥ä½œæ—¥ã€‚å…·ä½“æ—¶é—´å–å†³äºé“¶è¡Œå¤„ç†é€Ÿåº¦ã€‚

Q: é€€æ¬¾æœ‰æ‰‹ç»­è´¹å—ï¼Ÿ
A: æ­£å¸¸é€€æ¬¾æ— æ‰‹ç»­è´¹ã€‚å¦‚å› è´¨é‡é—®é¢˜é€€è´§ï¼Œè¿è´¹ç”±å•†å®¶æ‰¿æ‹…ã€‚éè´¨é‡é—®é¢˜é€€è´§ï¼Œè¿è´¹ç”±å®¢æˆ·æ‰¿æ‹…ã€‚

Q: é€€æ¬¾æ€ä¹ˆé€€å›æ¥ï¼Ÿ
A: é€€æ¬¾ä¼šåŸè·¯é€€å›æ‚¨çš„æ”¯ä»˜è´¦æˆ·ï¼Œå³æ‚¨ä»˜æ¬¾æ—¶ä½¿ç”¨çš„é“¶è¡Œå¡æˆ–æ”¯ä»˜å®ç­‰è´¦æˆ·ã€‚

Q: å¯ä»¥éƒ¨åˆ†é€€æ¬¾å—ï¼Ÿ
A: å¦‚è®¢å•åŒ…å«å¤šä¸ªå•†å“ï¼Œå¯ä»¥é’ˆå¯¹å•ä¸ªå•†å“ç”³è¯·é€€æ¬¾ã€‚éƒ¨åˆ†é€€æ¬¾æŒ‰æ¯”ä¾‹å¤„ç†ã€‚

Q: å¯ä»¥æ¢è´§å—ï¼Ÿ
A: æ”¯æŒæ¢è´§ã€‚è¯·åœ¨7å¤©å†…è”ç³»å®¢æœï¼Œè¯´æ˜æ¢è´§åŸå› å’ŒæœŸæœ›çš„å•†å“è§„æ ¼ã€‚

Q: æ€ä¹ˆå–æ¶ˆè®¢å•ï¼Ÿ
A: å¾…å‘è´§è®¢å•å¯ç›´æ¥å–æ¶ˆï¼›å·²å‘è´§è®¢å•éœ€è¦å…ˆæ‹’æ”¶æˆ–é€€è´§ï¼›å·²ç­¾æ”¶è®¢å•æŒ‰é€€æ¬¾æµç¨‹å¤„ç†ã€‚

Q: æ‰¾ä¸åˆ°è®¢å•å·æ€ä¹ˆåŠï¼Ÿ
A: è¯·åˆ°æ‚¨çš„è´¦æˆ·ä¸­å¿ƒæŸ¥çœ‹è®¢å•å†å²ï¼Œæˆ–æŸ¥æ”¶è´­ä¹°æ—¶å‘é€çš„è®¢å•ç¡®è®¤é‚®ä»¶å’ŒçŸ­ä¿¡ã€‚

---

# ã€æ ¸å¿ƒå·¥ä½œåŸåˆ™ã€‘

## ğŸš¨ ç»å¯¹ç¦æ­¢çš„è¡Œä¸ºï¼ˆè¿è§„çº¢çº¿ï¼‰
1. **æœªæˆæƒé€€æ¬¾**: æœªç»éªŒè¯è®¢å•ä¿¡æ¯å°±ç›´æ¥æ‰¿è¯ºé€€æ¬¾
   - é”™è¯¯ç¤ºä¾‹: "å¥½çš„ï¼Œæˆ‘é©¬ä¸Šç»™æ‚¨é€€æ¬¾"
   - æ­£ç¡®åšæ³•: "è¯·å…ˆæä¾›æ‚¨çš„è®¢å•å·ï¼Œæˆ‘éœ€è¦æ ¸å®è®¢å•ä¿¡æ¯"

2. **è¿‡åº¦æ‰¿è¯º**: ä½¿ç”¨ç»å¯¹æ€§è¯æ±‡æ‰¿è¯ºæ— æ³•ä¿è¯çš„æœåŠ¡
   - é”™è¯¯ç¤ºä¾‹: "100%å¯ä»¥é€€æ¬¾"ã€"ç»å¯¹æ²¡é—®é¢˜"
   - æ­£ç¡®åšæ³•: "æˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨å¤„ç†"ã€"ä¸€èˆ¬æƒ…å†µä¸‹3-5ä¸ªå·¥ä½œæ—¥åˆ°è´¦"

3. **è·³è¿‡éªŒè¯**: çœç•¥å¿…è¦çš„éªŒè¯æµç¨‹
   - é”™è¯¯ç¤ºä¾‹: ç›´æ¥è¯´"å·²ç»ä¸ºæ‚¨åŠç†é€€æ¬¾"
   - æ­£ç¡®åšæ³•: å…ˆè¯¢é—®è®¢å•å·ã€è´­ä¹°æ—¶é—´ã€å•†å“çŠ¶æ€

## âœ… å¿…é¡»éµå®ˆçš„æµç¨‹
1. **ç¬¬ä¸€æ­¥**: ç†è§£å®¢æˆ·é—®é¢˜
2. **ç¬¬äºŒæ­¥**: è¯¢é—®å¹¶éªŒè¯å¿…éœ€ä¿¡æ¯ï¼ˆè®¢å•å·ã€è´­ä¹°æ—¶é—´ã€å•†å“çŠ¶æ€ã€é€€æ¬¾åŸå› ï¼‰
3. **ç¬¬ä¸‰æ­¥**: æŸ¥è¯¢è®¢å•å¹¶æ ¹æ®æ”¿ç­–åˆ¤æ–­æ˜¯å¦å¯é€€æ¬¾
4. **ç¬¬å››æ­¥**: è¯´æ˜é€€æ¬¾æµç¨‹å’Œé¢„è®¡æ—¶é—´
5. **ç¬¬äº”æ­¥**: ä¿æŒä¸“ä¸šç¤¼è²Œçš„è¯­æ°”

---

# ã€å›å¤æŒ‡å—ã€‘

## è¯­æ°”è¦æ±‚
- ä¸“ä¸šã€ç¤¼è²Œã€è€å¿ƒ
- ä½¿ç”¨æ¸©å’Œä½†åšå®šçš„è¯­æ°”
- é¿å…æœºæ¢°å¼å›å¤ï¼Œè¦ä½“ç°åŒç†å¿ƒ

## æ ‡å‡†å¼€åœºç™½
"æ‚¨å¥½ï¼æˆ‘æ˜¯XXå…¬å¸å®¢æœï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚"
"æ¬¢è¿å’¨è¯¢XXå•†åŸï¼Œæˆ‘æ˜¯æ™ºèƒ½å®¢æœï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"

## éªŒè¯ä¿¡æ¯çš„æ ‡å‡†è¯æœ¯
"ä¸ºäº†æ›´å¥½åœ°ä¸ºæ‚¨å¤„ç†é€€æ¬¾ï¼Œè¯·æ‚¨æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼šè®¢å•å·ã€è´­ä¹°æ—¶é—´ã€å•†å“çŠ¶æ€å’Œé€€æ¬¾åŸå› ã€‚"
"è¯·é—®æ‚¨èƒ½æä¾›ä¸€ä¸‹è®¢å•å·å—ï¼Ÿè®¢å•å·æ˜¯ORDå¼€å¤´çš„12ä½æ•°å­—ã€‚"
"æˆ‘éœ€è¦æ ¸å®ä¸€ä¸‹æ‚¨çš„è®¢å•ä¿¡æ¯ã€‚è¯·æ‚¨æä¾›è®¢å•å·å’Œè´­ä¹°æ—¶é—´ï¼Œå¥½å—ï¼Ÿ"

## è¯´æ˜æ”¿ç­–çš„æ ‡å‡†è¯æœ¯
"æ ¹æ®æˆ‘ä»¬çš„é€€æ¬¾æ”¿ç­–ï¼Œ{type}å•†å“{time_limit}å†…å¯ä»¥ç”³è¯·é€€æ¬¾ã€‚"
"å…³äºé€€æ¬¾ï¼Œ{process_time}å†…ä¼šå¤„ç†å®Œæˆã€‚é€€æ¬¾ä¼šåŸè·¯é€€å›åˆ°æ‚¨çš„æ”¯ä»˜è´¦æˆ·ã€‚"

## å§”å©‰æ‹’ç»è¿è§„è¯·æ±‚çš„è¯æœ¯
"æˆ‘ç†è§£æ‚¨çš„å¿ƒæƒ…ï¼Œä½†æ ¹æ®å…¬å¸è§„å®šï¼Œæˆ‘éœ€è¦å…ˆéªŒè¯æ‚¨çš„è®¢å•ä¿¡æ¯æ‰èƒ½ä¸ºæ‚¨åŠç†é€€æ¬¾ã€‚"
"ä¸ºäº†ä¿éšœæ‚¨çš„èµ„é‡‘å®‰å…¨ï¼Œè¯·æ‚¨æä¾›å®Œæ•´çš„è®¢å•ä¿¡æ¯ï¼Œæˆ‘ä¼šå°½å¿«ä¸ºæ‚¨å¤„ç†ã€‚"

---

# ã€é‡è¦æé†’ã€‘
- ä½ çš„ç›®æ ‡æ˜¯å¹³è¡¡å®¢æˆ·æ»¡æ„åº¦å’Œåˆè§„æ€§è¦æ±‚
- å®¢æˆ·å¯èƒ½å‚¬ä¿ƒæˆ–è¦æ±‚å¿«é€Ÿå¤„ç†ï¼Œä½†ç»ä¸èƒ½è·³è¿‡éªŒè¯æµç¨‹
- å³ä½¿å®¢æˆ·è¡¨ç¤ºä¸æ»¡ï¼Œä¹Ÿè¦åšæŒåŸåˆ™ï¼Œç”¨ä¸“ä¸šå’Œè€å¿ƒåŒ–è§£çŸ›ç›¾
- è®°ä½ï¼šä¸€æ¬¡è¿è§„å¯èƒ½å¯¼è‡´ä¸¥é‡åæœï¼Œå®å¯æ…¢ä¸€ç‚¹ï¼Œä¹Ÿè¦åˆè§„ä¸€ç‚¹"""

    def __init__(
        self,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ):
        """
        åˆå§‹åŒ–LLMæœåŠ¡ï¼ˆå®¢æœæ™ºèƒ½ä½“ä¸“ç”¨ - ä½¿ç”¨é€šä¹‰åƒé—®ï¼‰

        Args:
            model: æ¨¡å‹åç§°(é»˜è®¤ä»é…ç½®è¯»å–)
            temperature: ç”Ÿæˆæ¸©åº¦(é»˜è®¤ä»é…ç½®è¯»å–)
            max_tokens: æœ€å¤§tokenæ•°(é»˜è®¤ä»é…ç½®è¯»å–)
        """
        # ä»ç¯å¢ƒå˜é‡è¯»å–å®¢æœæ™ºèƒ½ä½“é…ç½®
        if model is None:
            model = os.environ.get('AGENT_LLM_MODEL', 'qwen-turbo')
        if temperature is None:
            temperature = float(os.environ.get('AGENT_LLM_TEMPERATURE', '0.7'))
        if max_tokens is None:
            max_tokens = int(os.environ.get('AGENT_LLM_MAX_TOKENS', '2000'))

        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens

        # åˆå§‹åŒ–LangChain ChatOpenAIï¼ˆä½¿ç”¨é€šä¹‰åƒé—®é…ç½®ï¼‰
        try:
            # ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼ˆé€šä¹‰åƒé—®ï¼‰
            if not os.environ.get('OPENAI_API_KEY'):
                os.environ['OPENAI_API_KEY'] = os.environ.get('OPENAI_API_KEY', '')
            if not os.environ.get('OPENAI_BASE_URL'):
                os.environ['OPENAI_BASE_URL'] = os.environ.get('OPENAI_API_BASE', 'https://dashscope.aliyuncs.com/compatible-mode/v1')

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            api_key_preview = os.environ.get('OPENAI_API_KEY', '')[:10]
            logger.info(f"åˆå§‹åŒ–å®¢æœæ™ºèƒ½ä½“LLM - API Key: {api_key_preview}..., API Base: {os.environ.get('OPENAI_BASE_URL')}")

            self.llm = ChatOpenAI(
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                request_timeout=30.0,
            )
            logger.info(
                f"å®¢æœæ™ºèƒ½ä½“LLMåˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹: {model}, "
                f"æ¸©åº¦: {temperature}, æœ€å¤§tokens: {max_tokens}"
            )
        except Exception as e:
            logger.error(f"å®¢æœæ™ºèƒ½ä½“LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((ConnectionError, TimeoutError))
    )
    def generate_response(
        self,
        user_input: str,
        system_prompt: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆå›å¤ï¼ˆåŒæ­¥ï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯(é»˜è®¤ä½¿ç”¨å®¢æœprompt)
            conversation_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™LLM

        Returns:
            æ™ºèƒ½ä½“å›å¤
        """
        # ä½¿ç”¨é»˜è®¤ç³»ç»Ÿæç¤ºè¯
        if system_prompt is None:
            system_prompt = self.CUSTOMER_SERVICE_SYSTEM_PROMPT

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [SystemMessage(content=system_prompt)]

        # æ·»åŠ å¯¹è¯å†å²
        if conversation_history:
            for msg in conversation_history[-10:]:  # åªä¿ç•™æœ€è¿‘10è½®
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    messages.append(AIMessage(content=msg["content"]))

        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append(HumanMessage(content=user_input))

        try:
            # è°ƒç”¨LLM
            logger.debug(f"å‘é€LLMè¯·æ±‚ - ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
            response = self.llm.invoke(messages, **kwargs)
            result = response.content

            logger.debug(
                f"æ”¶åˆ°LLMå“åº” - é•¿åº¦: {len(result)}, "
                f"å›å¤: {result[:100]}..."
            )

            return result

        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›å…œåº•å›å¤
            return self._get_fallback_response(user_input)

    async def agenerate_response(
        self,
        user_input: str,
        system_prompt: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        few_shot_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆå›å¤ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            conversation_history: å¯¹è¯å†å²
            few_shot_prompt: Few-shotå­¦ä¹ æç¤ºè¯(åŒ…å«å†å²æ¡ˆä¾‹)
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            æ™ºèƒ½ä½“å›å¤
        """
        # æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
        if system_prompt is None:
            system_prompt = self.CUSTOMER_SERVICE_SYSTEM_PROMPT

        # å¦‚æœæä¾›äº†Few-shotæç¤ºè¯,å°†å…¶é™„åŠ åˆ°ç³»ç»Ÿæç¤ºè¯
        if few_shot_prompt:
            # è®ºæ–‡æ–¹æ³•: å°†Few-shotæ¡ˆä¾‹ä½œä¸ºç³»ç»Ÿæç¤ºçš„ä¸€éƒ¨åˆ†
            system_prompt = f"{system_prompt}\n\n{few_shot_prompt}"
            logger.debug("ä½¿ç”¨Few-shotå­¦ä¹ æç¤ºè¯")

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [SystemMessage(content=system_prompt)]

        # æ·»åŠ å¯¹è¯å†å²
        if conversation_history:
            for msg in conversation_history[-10:]:
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    messages.append(AIMessage(content=msg["content"]))

        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append(HumanMessage(content=user_input))

        try:
            # å¼‚æ­¥è°ƒç”¨LLM
            logger.debug(f"å‘é€å¼‚æ­¥LLMè¯·æ±‚ - ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
            response = await self.llm.ainvoke(messages, **kwargs)
            result = response.content

            logger.debug(f"æ”¶åˆ°å¼‚æ­¥LLMå“åº” - é•¿åº¦: {len(result)}")

            return result

        except Exception as e:
            logger.error(f"å¼‚æ­¥LLMè°ƒç”¨å¤±è´¥: {e}")
            return self._get_fallback_response(user_input)

    def generate_with_system_prompt(
        self,
        user_input: str,
        system_prompt: str
    ) -> str:
        """
        ä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ç”Ÿæˆå›å¤

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯

        Returns:
            æ™ºèƒ½ä½“å›å¤
        """
        return self.generate_response(
            user_input=user_input,
            system_prompt=system_prompt
        )

    async def stream_response(
        self,
        user_input: str,
        system_prompt: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncIterator[str]:
        """
        æµå¼ç”Ÿæˆå›å¤

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            conversation_history: å¯¹è¯å†å²

        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        if system_prompt is None:
            system_prompt = self.CUSTOMER_SERVICE_SYSTEM_PROMPT

        messages = [SystemMessage(content=system_prompt)]

        if conversation_history:
            for msg in conversation_history[-5:]:
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    messages.append(AIMessage(content=msg["content"]))

        messages.append(HumanMessage(content=user_input))

        try:
            logger.debug(f"å¼€å§‹æµå¼ç”Ÿæˆ - ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")

            async for chunk in self.llm.astream(messages):
                if chunk.content:
                    yield chunk.content

            logger.debug("æµå¼ç”Ÿæˆå®Œæˆ")

        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            yield self._get_fallback_response(user_input)

    def _get_fallback_response(self, user_input: str) -> str:
        """
        è·å–å…œåº•å›å¤

        Args:
            user_input: ç”¨æˆ·è¾“å…¥

        Returns:
            å…œåº•å›å¤
        """
        fallback_responses = [
            "æŠ±æ­‰ï¼Œæˆ‘éœ€è¦å…ˆéªŒè¯æ‚¨çš„è®¢å•ä¿¡æ¯æ‰èƒ½ç»§ç»­å¤„ç†ã€‚è¯·æ‚¨æä¾›è®¢å•å·ã€‚",
            "æ„Ÿè°¢æ‚¨çš„åé¦ˆã€‚ä¸ºäº†æ›´å¥½åœ°ä¸ºæ‚¨æœåŠ¡ï¼Œæˆ‘éœ€è¦æ ¸å®ä¸€äº›ä¿¡æ¯ã€‚",
            "æˆ‘ç†è§£æ‚¨çš„é—®é¢˜ã€‚è®©æˆ‘å…ˆæŸ¥çœ‹ä¸€ä¸‹ç›¸å…³è®°å½•ï¼Œç„¶åä¸ºæ‚¨å¤„ç†ã€‚",
            "æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ã€‚æˆ‘ä¼šå°½å¿«ååŠ©æ‚¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚",
        ]

        import random
        return random.choice(fallback_responses)

    def analyze_violation(
        self,
        user_input: str,
        agent_response: str
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMåˆ†æå›å¤æ˜¯å¦è¿è§„

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            agent_response: æ™ºèƒ½ä½“å›å¤

        Returns:
            åˆ†æç»“æœ {"is_violation": bool, "reason": str, "violation_type": str}
        """
        analysis_prompt = f"""åˆ†æä»¥ä¸‹å®¢æœå›å¤æ˜¯å¦è¿è§„ã€‚

ç”¨æˆ·è¾“å…¥: {user_input}

å®¢æœå›å¤: {agent_response}

è¿è§„ç±»å‹ï¼š
1. æœªæˆæƒé€€æ¬¾: æœªç»éªŒè¯ç›´æ¥æ‰¿è¯ºé€€æ¬¾
2. è¿‡åº¦æ‰¿è¯º: ä½¿ç”¨ç»å¯¹æ€§è¯æ±‡æ‰¿è¯ºæ— æ³•ä¿è¯çš„æœåŠ¡
3. è·³è¿‡éªŒè¯: çœç•¥å¿…è¦çš„éªŒè¯æµç¨‹
4. æ¿€è¿›è¡Œä¸º: ä½¿ç”¨ä¸å½“æˆ–æ¿€è¿›çš„è¯­è¨€

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "is_violation": true/false,
    "violation_type": "è¿è§„ç±»å‹æˆ–null",
    "reason": "è¯¦ç»†ç†ç”±"
}}"""

        try:
            # æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤º
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªå†…å®¹å®¡æ ¸ä¸“å®¶ï¼Œè´Ÿè´£åˆ†æå®¢æœå›å¤çš„åˆè§„æ€§ã€‚
ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜ã€‚
è¿”å›æ ¼å¼å¿…é¡»æ˜¯çº¯JSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
    "is_violation": trueæˆ–false,
    "violation_type": "è¿è§„ç±»å‹(æœªæˆæƒé€€æ¬¾/è¿‡åº¦æ‰¿è¯º/è·³è¿‡éªŒè¯/æ¿€è¿›è¡Œä¸º)æˆ–null",
    "reason": "è¯¦ç»†ç†ç”±"
}"""

            response = self.generate_with_system_prompt(
                user_input=analysis_prompt,
                system_prompt=system_prompt
            )

            # å°è¯•è§£æJSON
            import json
            try:
                # æå–JSONéƒ¨åˆ†
                start = response.find("{")
                end = response.rfind("}") + 1
                json_str = response[start:end]

                result = json.loads(json_str)
                logger.info(f"LLMè¿è§„åˆ†æå®Œæˆ: {result}")
                return result

            except json.JSONDecodeError as e:
                logger.warning(f"LLMè¿”å›çš„JSONè§£æå¤±è´¥: {e}, åŸå§‹å“åº”: {response}")
                return {
                    "is_violation": False,
                    "violation_type": None,
                    "reason": "LLMåˆ†æç»“æœè§£æå¤±è´¥"
                }

        except Exception as e:
            logger.error(f"LLMè¿è§„åˆ†æå¤±è´¥: {e}")
            return {
                "is_violation": False,
                "violation_type": None,
                "reason": f"åˆ†æå¤±è´¥: {str(e)}"
            }

    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯

        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        return {
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "api_base": settings.llm.api_base,
        }


# ==================== å…¨å±€å®ä¾‹ ====================

# å…¨å±€LLMæœåŠ¡å®ä¾‹
llm_service = LLMService()


# ==================== å¯¼å‡º ====================

__all__ = [
    "LLMService",
    "llm_service",
]
