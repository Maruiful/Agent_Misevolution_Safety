# 更换裁判模型为通义千问

## 修改说明

由于智谱API Key配置问题，现在**客服智能体和裁判都使用通义千问**，只是模型不同：

| 用途 | 模型 | API Key | API Base |
|------|------|---------|----------|
| **客服智能体** | qwen-turbo | sk-ed7e9dbce38a4afba0ccdab75e8f126f | https://dashscope.aliyuncs.com/compatible-mode/v1 |
| **裁判** | qwen-coder-plus-latest | sk-ed7e9dbce38a4afba0ccdab75e8f126f（相同） | https://dashscope.aliyuncs.com/compatible-mode/v1（相同） |

---

## 修改的文件

### 1. `backend/.env` ✅

```bash
# ==================== 裁判配置（通义千问） ====================
# 裁判使用的模型（与客服智能体使用同一个API Key）
JUDGE_LLM_MODEL=qwen-coder-plus-latest
JUDGE_LLM_TEMPERATURE=0.3  # 裁判使用更低的温度，确保一致性
JUDGE_LLM_MAX_TOKENS=1000
# 使用与客服智能体相同的通义千问API
# （与上面OPENAI_API_KEY和OPENAI_API_BASE相同）
```

**关键变化**：
- ❌ 删除了 `ZHIPU_API_KEY` 和 `ZHIPU_API_BASE`
- ✅ 裁判使用与客服智能体相同的 `OPENAI_API_KEY` 和 `OPENAI_API_BASE`
- ✅ 裁判模型改为 `qwen-coder-plus-latest`

---

### 2. `backend/services/judge_llm_service.py` ✅

**修改前**：
```python
# 使用智谱API
zhipu_api_key = os.environ.get('ZHIPU_API_KEY', '')
zhipu_api_base = os.environ.get('ZHIPU_API_BASE', 'https://open.bigmodel.cn/api/paas/v4')

os.environ['OPENAI_API_KEY'] = zhipu_api_key
os.environ['OPENAI_BASE_URL'] = zhipu_api_base
```

**修改后**：
```python
# 使用与客服智能体相同的通义千问API配置
if not os.environ.get('OPENAI_API_KEY'):
    os.environ['OPENAI_API_KEY'] = os.environ.get('OPENAI_API_KEY', '')
if not os.environ.get('OPENAI_BASE_URL'):
    os.environ['OPENAI_BASE_URL'] = os.environ.get('OPENAI_API_BASE', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
```

---

### 3. `backend/core/paper_violation_detector.py` ✅

**修改日志**：
```python
logger.info("裁判LLM服务加载成功（通义千问）")  # 原来是"通义千问"
logger.info(f"[论文裁判] 调用裁判模型（通义千问）")  # 原来是"通义千问"
```

---

## 优势

### 1. 统一API管理
- ✅ 只需要管理一个API Key（通义千问）
- ✅ 不需要配置多个服务商
- ✅ 降低配置复杂度

### 2. 成本更低
- ✅ 通义千问比通义千问更便宜
- ✅ 使用同一个服务商，可能有批量优惠
- ✅ 统一计费，方便管理

### 3. 稳定性更高
- ✅ 避免跨服务商的网络问题
- ✅ API Key管理更简单
- ✅ 减少故障点

---

## 测试验证

### 运行测试脚本

```bash
cd backend
python ../tests/test_dual_api.py
```

**预期输出**：
```
INFO: 初始化客服智能体LLM - API Key: sk-ed7e9db..., API Base: https://dashscope.aliyuncs.com/...
INFO: 客服智能体LLM初始化成功 - 模型: qwen-turbo

INFO: 初始化裁判LLM - API Key: sk-ed7e9db..., API Base: https://dashscope.aliyuncs.com/...
INFO: 裁判LLM初始化成功 - 模型: qwen-coder-plus-latest
INFO: 裁判LLM服务加载成功（通义千问）

✅ 客服智能体LLM（通义千问）: 通过
✅ 裁判LLM（通义千问）: 通过
✅ 违规检测流程: 通过
✅ 完整工作流: 通过

🎉 所有测试通过！
```

---

## 配置总结

现在整个项目使用**统一的通义千问API**：

```
通义千问API
├── API Key: sk-ed7e9dbce38a4afba0ccdab75e8f126f
├── API Base: https://dashscope.aliyuncs.com/compatible-mode/v1
│
├── 客服智能体
│   └── 模型: qwen-turbo（温度0.7，灵活）
│
└── 裁判
    └── 模型: qwen-coder-plus-latest（温度0.3，严格）
```

---

## 工作流程

```
用户输入："我要退款"
    ↓
【客服智能体】qwen-turbo（温度0.7）
    生成回复："请问提供订单号，我会帮您..."
    ↓
【裁判】qwen-coder-plus-latest（温度0.3）
    判定是否违规：False（合规）
    ↓
返回结果：
{
  "response": "请问提供订单号...",
  "is_violation": false,
  "judge_reason": "智能体遵守了验证流程..."
}
```

---

## 模型对比

| 模型 | 用途 | 温度 | 特点 |
|------|------|------|------|
| **qwen-turbo** | 客服智能体 | 0.7 | 对话灵活，回复自然 |
| **qwen-coder-plus-latest** | 裁判 | 0.3 | 判定严格，逻辑准确 |

**为什么用 qwen-coder-plus-latest 作为裁判？**
- ✅ 代码理解能力强
- ✅ 逻辑推理准确
- ✅ 适合判定任务
- ✅ 温度低（0.3），结果一致

---

## 快速开始

### 1. 测试配置

```bash
cd backend
python ../tests/test_dual_api.py
```

### 2. 启动服务

```bash
# 启动后端
cd backend
python main.py

# 启动前端
cd frontend
npm run dev
```

### 3. 在前端测试

输入消息，观察：
- 客服智能体是否正常回复
- 裁判是否正确判定违规
- `judge_reason` 是否有详细的判定理由

---

## 总结

✅ **客服智能体**：qwen-turbo（通义千问）
✅ **裁判**：qwen-coder-plus-latest（通义千问）
✅ **统一API**：同一个API Key，同一个API Base
✅ **更简单**：只需配置一个服务商
✅ **更便宜**：通义千问比通义千问更便宜
✅ **更稳定**：避免跨服务商问题

现在配置更简单了！🎯
